{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f1ba1f9-93f2-4292-82a0-e4d34de44d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3da1209-be6d-4e8e-9a06-5828c1261ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50f1e4bc-a108-4a4b-ab26-72a19274725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ece2c098-a79e-4dba-a5b4-51dc75ee6d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c6885dc-1440-4b1a-8a64-74f1586287a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "unique_classes = df['Category'].unique()\n",
    "print(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45969210-d9ed-41c4-8c9d-addcddec936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "0    4825\n",
      "1     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_sizes = df['Category'].value_counts()\n",
    "print(class_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85d66847-550e-4052-988e-87ce3f72032c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df['Category'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e06ed251-a366-44e6-9aa8-e650e1aabc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1045\n",
      "Validation set size: 224\n",
      "Test set size: 225\n"
     ]
    }
   ],
   "source": [
    "# Separate the majority and minority classes\n",
    "df_majority = df[df['Category'] == 0]\n",
    "df_minority = df[df['Category'] == 1]\n",
    "\n",
    "# Downsample the majority class\n",
    "df_majority_downsampled = df_majority.sample(n=len(df_minority), random_state=42)\n",
    "\n",
    "# Combine the minority class with the downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Shuffle the resulting DataFrame\n",
    "df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the downsampled DataFrame into train (70%) and temp (30%) sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(\n",
    "    df_downsampled['Message'], df_downsampled['Category'], \n",
    "    random_state=0, \n",
    "    test_size=0.3, \n",
    "    stratify=df_downsampled['Category']\n",
    ")\n",
    "\n",
    "# Split the temp set into validation (50% of temp, i.e., 15% of total) and test (50% of temp, i.e., 15% of total) sets\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(\n",
    "    temp_text, temp_labels, \n",
    "    random_state=0, \n",
    "    test_size=0.5, \n",
    "    stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", len(train_text))\n",
    "print(\"Validation set size:\", len(val_text))\n",
    "print(\"Test set size:\", len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "146ea56b-a72f-44f5-9c9a-944afd8be8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd89c5d17be14913a86a5af280accb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Abhishek\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5aa4cba7ea4a968dafefff1baf988b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78f8df74bc44db9b7bb280c7a08679a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b5443cd22a484aaa0076b7c61b7723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70da93f903c9462d80e9612bc1982966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eae1bf6b-f527-4505-a1eb-b8e7bbf8e795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "#X-axis message size and y_axis freq\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db1b30f7-73eb-4595-b13d-acaf75d687f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "586bedc3-5669-4dc8-9e24-de577b4c4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d40b25e-8838-4681-95e6-2eae47f27eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efe0d626-06f0-4787-8041-a44b4601a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d6a15aa-65f7-41b5-baf7-156e3af39611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d056d637-34f6-4731-98e8-393298c9abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c1dd402-300d-46a1-ab2c-62be97953d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f70fe384-6ad1-4f73-bc28-75ef9d298c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.00095785 0.99904398]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming train_labels is a pandas Series, convert it to a numpy array\n",
    "train_labels_array = train_labels.to_numpy()\n",
    "\n",
    "# Compute the class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels_array),\n",
    "    y=train_labels_array\n",
    ")\n",
    "\n",
    "print('Class Weights:', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc44fdeb-0085-4b8f-939d-91d46335c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53cac110-bf7f-4230-9d5f-16fcd6940269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        # batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34eb47ea-c151-4079-9725-7f6e7749a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        # batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "219718f6-8179-4518-b8ee-5caea4e75825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 1\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.675\n",
      "Validation Loss: 0.664\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "#defining epochs\n",
    "epochs = 1\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "591299c0-ae8b-4ee3-9b0a-36c536324476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best modelpath = 'saved_weights.pt'model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5727479-33b7-46de-99c0-3120eeead22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.78       113\n",
      "           1       0.76      0.84      0.80       112\n",
      "\n",
      "    accuracy                           0.79       225\n",
      "   macro avg       0.79      0.79      0.79       225\n",
      "weighted avg       0.79      0.79      0.79       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq, test_mask)\n",
    "    preds = preds.detach().numpy()\n",
    "\n",
    "\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
